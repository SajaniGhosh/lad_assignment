{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3576606-f6ea-4a3e-b21b-5484932c1ecd",
   "metadata": {},
   "source": [
    "# Data Augmention using Generative LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7f654e-7425-440e-8508-9701b6a524f5",
   "metadata": {},
   "source": [
    "<center><img src=\"https://media.licdn.com/dms/image/v2/D4E12AQGLk5R0lcfr8A/article-cover_image-shrink_720_1280/article-cover_image-shrink_720_1280/0/1713883053110?e=1737590400&v=beta&t=0b0YHpIGPl8KxMzLfEhhwFbkcrT3ZLz2BDTaax-4HNM\" alt=\"llama\" style=\"width:60%\"></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ea6af3-c56c-42dc-bfc6-3b6865565021",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br>\n",
    "### Setting things up:\n",
    "\n",
    "First, we need to set some things up. This usually starts by **creating a virtual environment**.\n",
    "\n",
    "Open your preferred command line. Make sure **Python** is available. This means you should be able to enter the Python promp if you type ```Python```.\n",
    "\n",
    "**Navigate/create a folder** where you want to run your system. \n",
    "\n",
    "Inside that folder you can run the following commands:\n",
    "\n",
    "```\n",
    "conda deactivate           # deactivate conda in case you have it active\n",
    "python -m venv venv       # create a folder 'venv' that will contain your virtual environment\n",
    "source venv/bin/activate   # activate your virtual environment; you should now see '(venv)' on the left of your terminal prompt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a24fa13-3ba2-4b5c-9218-ce7919e7cd22",
   "metadata": {},
   "source": [
    "You are now inside your clean virtual enviroment: nothing except python should be available. \n",
    "**Let's start installing the necessary things**!\n",
    "\n",
    "**Run the following commands** inside your terminal:\n",
    "\n",
    "```\n",
    "pip install llama-cpp-python\n",
    "pip install openai\n",
    "pip install sse_starlette\n",
    "pip install starlette_context\n",
    "pip install pydantic_settings\n",
    "pip install \"fastapi[all]\"\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "We should everything we need to run LLMs locally...  We're almost there! \n",
    "\n",
    "<br><br>\n",
    "\n",
    "#### What are we missing?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37279c7f-6768-4498-a7f9-6cc3fb1bda8d",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "# An LLM!  \n",
    "### (Where can we find them? Aren't they huge?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6a5a4-3747-4a00-bb1a-5d1b791f3c41",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e81f6-23c0-4085-82fd-1b6be5c23cb5",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "#### What is it? \n",
    "[Read more here](https://medium.com/@techresearchspace/what-is-quantization-in-llm-01ba61968a51)\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58084b4c-ef4d-4e94-abe0-4af6f43d19d4",
   "metadata": {},
   "source": [
    "**Advantages:**\n",
    " - Lesser memory consumption: Lower bit width results in less memory for storage\n",
    " - Fast Inference: This is due to efficient computation due to its lower memory bandwidth requirements\n",
    " - Less energy consumption: Larger model need more data movement and storage resulting in more energy consumption. Hence a smaller model results in compartively lesser energy usage.\n",
    " - Smaller models: Can quantize to suit the need and deploy to device with samller hadware specifications.\n",
    "\n",
    "**Disadvantages:**\n",
    " - Potential loss in accuracy due to squeezing of high precision weight to lower precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dbf681-b707-43fd-a395-4b75896de0d3",
   "metadata": {},
   "source": [
    "### We'll be using Quantized LLMs, in GGUF format\n",
    "[Click here to know more about and how to find GGUF models](https://huggingface.co/docs/hub/en/gguf)\n",
    "\n",
    "[QuantFactory](https://huggingface.co/QuantFactory) is a good place to start!\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br><br><br>\n",
    "\n",
    "### Once you download your GGUF model, you should be able to run it (from within the virtual environment):\n",
    "\n",
    "`\n",
    "python -m llama_cpp.server --host 0.0.0.0 --model ./models/Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf --n_ctx 4096\n",
    "`\n",
    "\n",
    "This launches the **LLM as a server** that **can be used while it is running**.\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ec47a-4ac7-41b4-9617-df4c2fe7ae37",
   "metadata": {},
   "source": [
    "# Using the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa848bce-082d-419b-a47d-147bf77424da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from collections import defaultdict as dd\n",
    "\n",
    "# Point to the server\n",
    "client = OpenAI(base_url=\"http://localhost:9000/v1\", api_key=\"cltl\")\n",
    "\n",
    "#client = OpenAI(base_url=\"http://130.37.53.128:9002/v1\", api_key=\"cltl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2a7bb-62b9-4630-954c-38feb744c07d",
   "metadata": {},
   "source": [
    "## 1) In chat mode\n",
    "- Try to understand how the contents of `history` is used\n",
    "- What are the different `\"roles\"`?\n",
    "- How to stop the chat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8b4c692-3c98-4f2c-bc43-f347671eff26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I assist you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  With nothing. Thank you.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Feel free to reach out if you need anything in the future. Have a great day!\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " BYE BYE!\n"
     ]
    }
   ],
   "source": [
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"You an chat-based assistant. Be cooperative, and polite. Try to be concise.\"},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"local-model\",\n",
    "        messages=history,\n",
    "        temperature=0.8,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "\n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "            new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "    history.append(new_message)\n",
    "    print(\"\\n\")\n",
    "    userinput = input(\"> \")\n",
    "    if userinput.lower() in [\"bye\", \"quit\", \"exit\"]:\n",
    "        print(\"\\n BYE BYE!\")\n",
    "        break\n",
    "    history.append({\"role\": \"user\", \"content\": userinput})\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da197b1-e1f2-457e-be54-b5a3257b65df",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad7021-1729-4f39-97a7-40390339f564",
   "metadata": {},
   "source": [
    "## 2) In single prompt mode\n",
    "- Is `history` still used?\n",
    "- What does temperature do? What should theoretically happen if we set it to 0?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "672789e9-7c5f-4018-b7b7-4dfac67790e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_LLM(model_client, prompt, temp=0.6):\n",
    "    history = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "    ]\n",
    "    completion = model_client.chat.completions.create(\n",
    "        model=\"local-model\", # this field is currently unused\n",
    "        messages=history,\n",
    "        temperature=temp,\n",
    "        stream=False,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2fb6855-dc90-45cc-8625-81fdd6dacca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering, also known as prompt design or prompt optimization, is the process of crafting and refining input prompts to elicit specific, accurate, and informative responses from language models. It involves understanding how language models generate text based on inputs and designing optimal prompts that guide them towards desired outputs.\n",
      "\n",
      "The goal of prompt engineering is to improve the quality and relevance of a model's responses by making its internal workings more interpretable and controllable. This requires a combination of natural language processing (NLP) expertise, domain knowledge, and understanding of how language models process text.\n",
      "\n",
      "Prompt engineering has several applications across various industries:\n",
      "\n",
      "1. **Chatbots and Virtual Assistants**: Crafting prompts to elicit specific information or actions from users.\n",
      "2. **Language Translation**: Designing prompts for language translation systems to capture nuances and context.\n",
      "3. **Text Summarization**: Creating prompts that guide summarization models to focus on key points and omit irrelevant details.\n",
      "4. **Question Answering (QA) Systems**: Developing prompts to elicit specific, accurate answers from QA models.\n",
      "5. **Creative Writing and Content Generation**: Using prompts to inspire creative writing or generate content with specific styles or tones.\n",
      "\n",
      "To engineer effective prompts, one must consider the following factors:\n",
      "\n",
      "1. **Clarity**: Ensure that the prompt is clear and concise.\n",
      "2. **Specificity**: Provide enough context for the model to understand what's being asked.\n",
      "3. **Relevance**: Make sure the prompt aligns with the model's capabilities and knowledge domain.\n",
      "4. **Contextual understanding**: Consider the model's ability to comprehend nuances, idioms, or figurative language.\n",
      "5. **Evaluability**: Design prompts that allow for easy evaluation of their effectiveness.\n",
      "\n",
      "By applying these principles, developers can optimize prompts to unlock the full potential of language models and improve overall performance in various applications.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is prompt engineering?\"\n",
    "print(query_LLM(client, prompt, temp=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7a192cab-ecf9-4436-980e-a0ac1f34db3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coffee.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is the best drink in the world? Your answer should contain the name of a single drink and nothing more.\"\n",
    "print(query_LLM(client, prompt, temp=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb9884e-8bda-4a85-b9e0-4dd278376fad",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d52238-7918-406a-a6ce-0392f6d30ee6",
   "metadata": {},
   "source": [
    "# Data Augmentation -- Machine Translation as a case study\n",
    "1) We must know what we want\n",
    "2) We must know if we can/how to ask it\n",
    "3) We must learn how to process its output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b17422-516d-4ab1-9494-6c8226b91a90",
   "metadata": {},
   "source": [
    "## 1) Who can help me understand what you need for your assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484fa451-3ee6-41dc-b38e-69af80c16c95",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d872c48d-c28d-4e66-adf7-6c0bdb57dd82",
   "metadata": {},
   "source": [
    "## 2) We must know if we can/how to ask it\n",
    "- Am I using the right LLM for the task?\n",
    "- Should we be using the chat or the single prompt method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fc79869-ec32-452c-a618-45d465515e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can translate text from one language to another. I support translations in many languages, including but not limited to:\n",
      "\n",
      "* European languages: English, Spanish, French, German, Italian, Portuguese, Dutch, Russian, Chinese (Simplified and Traditional), Japanese, Korean\n",
      "* Indian languages: Hindi, Bengali, Marathi, Gujarati, Punjabi, Tamil, Telugu, Malayalam, Kannada\n",
      "* Middle Eastern languages: Arabic, Hebrew, Persian (Farsi)\n",
      "* Asian languages: Thai, Vietnamese, Indonesian, Malaysian\n",
      "\n",
      "To translate text, you can:\n",
      "\n",
      "1. Type or paste the text into this chat window.\n",
      "2. Specify the source language and target language.\n",
      "\n",
      "For example, you can say \"Translate 'Hello' from English to Spanish\" or simply type \"translate hello en es\".\n",
      "\n",
      "Keep in mind that my translation abilities are based on machine learning algorithms, so while I strive for accuracy, there may be occasional errors or nuances that get lost in translation. If you need a professional or high-stakes translation, it's always best to consult a human translator.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you translate text?\"\n",
    "# prompt = \"What languages were you trained with?\"\n",
    "print(query_LLM(client, prompt, temp=0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f47a7-6fdf-46bb-8822-2ce36bad99b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8ee642-1cd8-453a-a5a8-146bcd6c8cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73e38c22-5331-4401-b462-9d582feb60e1",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed28c376-3d94-4c19-b171-1442b5463fab",
   "metadata": {},
   "source": [
    "## 3) We must learn how to process its output\n",
    "- How do we stop all the yapping?\n",
    "- What is the difference between zero-/one-/few-shot prompting?\n",
    "- How do we get the ouput in the format we want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a49334-fce9-4b79-9666-a75e9cfcee0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e2cfb79-5a38-4b65-912d-564ad6c393a6",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "191d3f25-6c36-4f9e-9aad-b3a3baeee91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "DONE:\n",
      "['The company is between a rock and a hard place with these financial decisions.', 'A empresa está entre uma pedra e um morro com essas decisões financeiras.']\n",
      "\n",
      "\n",
      "[\"He's burning the midnight oil to meet this project deadline, but it's worth it.\", 'Ele está queimando o óleo da meia-noite para cumprir a data-limite desse projeto, mas vale a pena.']\n",
      "\n",
      "\n",
      "['The new policy is a double-edged sword for our business, both positive and negative impacts.', 'A nova política é uma espada de dois gumes para nossa empresa, tanto impactos positivos quanto negativos.']\n",
      "\n",
      "\n",
      "[\"After the scandal, the company's reputation was left in tatters, a complete loss of trust.\", 'Depois do escândalo, a reputação da empresa ficou em frangalhos, uma perda completa de confiança.']\n",
      "\n",
      "\n",
      "['The team is feeling under the weather after that grueling project, they need some rest.', 'O time está se sentindo mal de saúde após aquele projeto agoniante, eles precisam de um pouco de descanso.']\n",
      "\n",
      "\n",
      "['The company is between a rock and a hard place with these financial decisions.', 'A empresa está entre uma pedra e um muro com essas decisões financeiras.']\n",
      "\n",
      "\n",
      "[\"He's burning the midnight oil to meet this deadline, but it's not going well.\", 'Ele está queimando o óleo da meia-noite para cumprir essa data-limite, mas não está indo bem.']\n",
      "\n",
      "\n",
      "['The new policy is a double-edged sword for our company, bringing both benefits and drawbacks.', 'A nova política é uma espada de dois gumes para nossa empresa, trazendo tanto benefícios quanto desvantagens.']\n",
      "\n",
      "\n",
      "['The team is feeling under the weather after that tough loss in the championship game.', 'O time está se sentindo mal de saúde após aquela derrota difícil na final do campeonato.']\n",
      "\n",
      "\n",
      "['The new employee is a fish out of water in this fast-paced and competitive work environment.', \"O novo funcionário é um peixe fora d'água nesse ambiente de trabalho acelerado e competitivo.\"]\n",
      "\n",
      "\n",
      "['The company is between a rock and a hard place due to financial difficulties.', 'A empresa está entre uma pedra e um morro devido às dificuldades financeiras.']\n",
      "\n",
      "\n",
      "[\"He's burning the midnight oil to meet the deadline for his project.\", 'Ele está queimando o óleo da meia-noite para cumprir o prazo do seu projeto.']\n",
      "\n",
      "\n",
      "['The new policy is a double-edged sword that can either help or harm the company.', 'A nova política é uma espada de dois gumes que pode ajudar ou prejudicar a empresa.']\n",
      "\n",
      "\n",
      "['The team is on the same page regarding their strategy for the upcoming election.', 'O time está na mesma página em relação à sua estratégia para as próximas eleições.']\n",
      "\n",
      "\n",
      "[\"The company's financial situation is a ticking time bomb that could explode at any moment.\", 'A situação financeira da empresa é uma bomba-relógio que pode explodir a qualquer momento.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Working Example (Make sure you understand why we use a WHILE loop, and why we must use the TRY statement;) \n",
    "prompt = \"\"\"\n",
    "Please provide 5 english sentences and their respective portuguese translations. Each English sentence should be between 10 and 15 words long and must contain an idiom.\n",
    "Your answer should be a list of list in python. The first element of each list should contain the English sentence and the second element should contain the Portuguese translation.\n",
    "Provide only the list and nothing else. For example:\n",
    "[[\"english sentence\"], [\"portuguese translation], ...]\n",
    "\"\"\"\n",
    "\n",
    "mt_list = []\n",
    "\n",
    "import json\n",
    "\n",
    "while len(mt_list) < 15:\n",
    "    answer = query_LLM(client, prompt, temp=0.2)\n",
    "\n",
    "    try:\n",
    "        a = json.loads(answer)\n",
    "\n",
    "        for item in a:\n",
    "            mt_list.append(item)\n",
    "    except:\n",
    "        print(\"Failed to parse: \", answer)\n",
    "        print(\"Trying again!\\n\\n\\n\")\n",
    "\n",
    "print(\"\\n\\n\\nDONE:\")\n",
    "for pair in mt_list:\n",
    "    print(pair)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752c226-fbef-4110-96dd-a5eefc4c3382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c5e1ec2-a17c-4950-ae0e-6c8bef1e3731",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d158d3-94ba-419c-b5a3-fe43cf92ac64",
   "metadata": {},
   "source": [
    "# Quality Estimation / \"Evaluation\" with BLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8432c9-dd2f-4cb0-8df8-e22af0d23dbe",
   "metadata": {},
   "source": [
    "<center><img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEhdPDCCJH2WVAuEWUvp-RWdQITk9L8dB2p62GVI9CzLHd_hC2cED4wovkTY07sSZmYHtiWcHbSUhPRzbg_2DYyHiq_9gElMN85ZwZAI2gPcuwQNleQATdqUlrd8klzjOLhvh-weaAWdqkA2/s1600/BLEU4.png\" alt=\"llama\" style=\"width:90%\"></a><br><a href=\"https://kv-emptypages.blogspot.com/2019/04/understanding-mt-quality-bleu-scores.html\">Taken from/Read more here</a></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf8f5af-9517-4141-9cba-89c93a1a1dcc",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e6c4a0-7f8c-42bc-8edc-7edc96cddb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28504467-e1aa-4112-a9fa-9745c3df484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/lmc/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5c2c008-1081-4d6b-b015-5b069aabb75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the',), ('teacher',), ('arrived',), ('late',), ('because',), ('of',), ('the',), ('traffic',), ('the', 'teacher'), ('teacher', 'arrived'), ('arrived', 'late'), ('late', 'because'), ('because', 'of'), ('of', 'the'), ('the', 'traffic'), ('the', 'teacher', 'arrived'), ('teacher', 'arrived', 'late'), ('arrived', 'late', 'because'), ('late', 'because', 'of'), ('because', 'of', 'the'), ('of', 'the', 'traffic'), ('the', 'teacher', 'arrived', 'late'), ('teacher', 'arrived', 'late', 'because'), ('arrived', 'late', 'because', 'of'), ('late', 'because', 'of', 'the'), ('because', 'of', 'the', 'traffic')]\n"
     ]
    }
   ],
   "source": [
    "source_sent = \"le professeur est arrivé en retard à cause de la circulation\"\n",
    "reference_transl = \"the teacher arrived late because of the traffic\"\n",
    "reference_transl_tok = nltk.word_tokenize(reference_transl)\n",
    "ngrams_reference = []\n",
    "for n in [1,2,3,4]:\n",
    "    ngrams_reference = ngrams_reference + list(ngrams(reference_transl_tok,n))\n",
    "\n",
    "print(ngrams_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89839c6d-acf3-4048-b3ae-471c3d3c592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'professor', 'was', 'delayed', 'due', 'to', 'the', 'congestion'], ['Congestion', 'was', 'responsible', 'for', 'the', 'teacher', 'being', 'late'], ['The', 'teacher', 'was', 'late', 'due', 'to', 'the', 'traffic'], ['The', 'professor', 'arrived', 'late', 'because', 'circulation'], ['The', 'teacher', 'arrived', 'late', 'because', 'of', 'the', 'traffic']]\n"
     ]
    }
   ],
   "source": [
    "transl_list = [\n",
    "    \"The professor was delayed due to the congestion\",\n",
    "    \"Congestion was responsible for the teacher being late\",\n",
    "    \"The teacher was late due to the traffic\",\n",
    "    \"The professor arrived late because circulation\",\n",
    "    \"The teacher arrived late because of the traffic\"\n",
    "]\n",
    "\n",
    "transl_list_tokenized = []\n",
    "for sent in transl_list:\n",
    "    transl_list_tokenized.append(nltk.word_tokenize(sent))\n",
    "\n",
    "print(transl_list_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c87278c-c738-4d4f-8c4e-29648aa06f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0832677820940877e-231 1.052691193011681e-277\n",
      "{('the',)}\n",
      "\n",
      "\n",
      "\n",
      "7.176381577237209e-155 1.2950316234712509e-185\n",
      "{('the', 'teacher'), ('teacher',), ('late',), ('the',)}\n",
      "\n",
      "\n",
      "\n",
      "7.711523862191631e-155 1.3328284280434942e-185\n",
      "{('the',), ('teacher',), ('late',), ('traffic',), ('the', 'traffic')}\n",
      "\n",
      "\n",
      "\n",
      "4.1382219658909647e-78 1.695647221393335e-93\n",
      "{('arrived', 'late'), ('arrived', 'late', 'because'), ('because',), ('late',), ('late', 'because'), ('arrived',)}\n",
      "\n",
      "\n",
      "\n",
      "0.8408964152537145 0.834236890454548\n",
      "{('teacher', 'arrived'), ('of', 'the', 'traffic'), ('late', 'because', 'of', 'the'), ('the',), ('late', 'because'), ('because', 'of'), ('arrived', 'late', 'because', 'of'), ('arrived', 'late'), ('teacher', 'arrived', 'late', 'because'), ('teacher', 'arrived', 'late'), ('because', 'of', 'the', 'traffic'), ('arrived',), ('late', 'because', 'of'), ('because', 'of', 'the'), ('late',), ('of', 'the'), ('because',), ('arrived', 'late', 'because'), ('teacher',), ('traffic',), ('the', 'traffic'), ('of',)}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "ngram_weights = (0.10, 0.30, 0.30, 0.30) # weights for 1-gram, 2-gram, 3-gram, 4-gram\n",
    "\n",
    "for translation in transl_list_tokenized:\n",
    "\n",
    "    # Fine the translation n-grams\n",
    "    # Not needed for the score, just to see the overlap\n",
    "    sent_ngrams = []\n",
    "    for n in [1,2,3,4]:\n",
    "        sent_ngrams = sent_ngrams + list(ngrams(translation,n))\n",
    "    \n",
    "    \n",
    "    bleu_score1 = sentence_bleu([reference_transl_tok], translation)  # This can be a list of references \n",
    "    bleu_score2 = sentence_bleu([reference_transl_tok], translation, weights=ngram_weights) # This can be a list of references \n",
    "\n",
    "    print(bleu_score1, bleu_score2)\n",
    "\n",
    "    #1-gram overlap\n",
    "    print(set(ngrams_reference) & set(sent_ngrams))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342171e5-87e5-4c68-8b4f-d7e77bb54e07",
   "metadata": {},
   "source": [
    "## Why is the last one not 1.0?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
